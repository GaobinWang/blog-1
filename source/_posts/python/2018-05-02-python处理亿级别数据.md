
---
title: python处理亿级别数据
date: 2018-05-02 22:05:46
tags: [python,大数据,IO,pandas]
mathjax: true
---

在利用python做数据分析时，我们往往会遇到这样的状况：数据量太大以至于我们无法将其读入python中进行进一步分析，甚至无法用文本编辑器将其打开查看数据的结构。本文针对上述状况状况提出一个解决方案，用到的主要方法有：
- open函数和readline函数
- pandas中的分块读取和分块写出操作
- 利用pandas中DataFrame的数据结构进行优化

由于网上针对该类问题的博客中用到的数据获取成本较高，因此本文给出一种模拟数据的生成方法，并针对生成的模拟数据进行分析。

# 生成模拟数据

我们利用随机数生成某金融产品的秒级别行情数据，主要包括六个字段：时间（datetime）、产品代码（stock_code）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）。

此处我们假设该金融产品全天24小时交易，且没有节假日。

在生成模拟数据的函数GenerateData中，参数days代表生成多少天的数据，参数stock_num代表金融产品的个数。生成随机数据时，days和stock_num数值的选取应该根据自己电脑的配置进行合理选择。

在本案例中，我们的电脑配置为：
- 处理器：英特尔 Core i5-6300HQ @ 2.30GHz 四核
- 内存：12 GB ( 金士顿 DDR4 2400MHz / 镁光 DDR4 2400MHz )

因此我们取days=100,stock_num=30,此时生成的模拟数据simulation_data.txt的大小约为23.5G，共包括约2.6亿条记录。生成数据大约需要30min。


```python
###加载相关库
import os
import time
import sys
import numpy as np
import pandas as pd
from datetime import datetime
from datetime import timedelta

###设置工作路径
path="E:\\python学习\\python性能优化"
os.chdir(path)
```


```python
def GenerateData(days,stock_num):
    data_size_oneday = 24*60*60 #hours*minutes*seconds
    for day in range(days):
        theday=datetime.now() - timedelta(days=days-day)
        theday=theday.strftime(format="%Y%m%d")
        for stock_code in range(1,stock_num):
            mu = stock_code
            sigma = stock_code/10
            data = {'datetime':pd.date_range(theday,periods = data_size_oneday,freq='1S'),
                    'stock_code':[stock_code]*data_size_oneday,
                    'open':np.random.normal(mu, sigma, data_size_oneday),
                    'high':np.random.normal(mu, sigma, data_size_oneday),
                    'low':np.random.normal(mu, sigma, data_size_oneday),
                    'close':np.random.normal(mu, sigma, data_size_oneday)}
            df = pd.DataFrame(data,columns=['datetime','stock_code','open','high','low','close'])
            df.to_csv("simulation_data.txt",sep="\t",mode='a',header=False,index=False)
            print("day:%d ### stock_code:%s" % (day,stock_code))
#time1=time.time()
#GenerateData(days,stock_num)
#time2=time.time()
#print("生成数据共需要:%.2f 秒" % (time2-time1))
```

# 探索数据

在实际的数据分析过程中，我们往往对实际数据一无所知。受到计算机内存的限制，我们也无法将数据一次性读入内存中。

接下来，我们利用open、readline两个函数来查看23.5G的数据文件中有哪些列，并计算数据共有多少行。


```python
###查看数据的前5行
f=open("simulation_data.txt","r")
for i in range(5):
    line=f.readline()
    print(line)
f.close()
```

    2018-01-20 00:00:00	1	1.0242841282525321	1.2444348000116294	0.9598490132510167	0.9594019697150978
    
    2018-01-20 00:00:01	1	0.9967798481269038	0.893297617253066	0.9249188962462151	0.9339331914282224
    
    2018-01-20 00:00:02	1	0.927216953105946	1.1581285233473535	0.8781121478624692	0.9951808299218182
    
    2018-01-20 00:00:03	1	1.1147894272679106	0.9522958167403974	1.103172699025737	0.8830839054472519
    
    2018-01-20 00:00:04	1	1.0485937697222474	1.0690999288043306	1.112799534092575	1.000219599323638
    
    


```python
###计算数据共有多少行
time1=time.time()
lines=0
with open("simulation_data.txt","r") as f:
    for line in f:
        lines+=1
    print(lines)
time2=time.time()
print("time:%.2f s" %(time2-time1)) 
```

    259200000
    time:297.93s
    

从上面的例子可以看出，23.5G的文件共包含259200000行数据（近2.6亿行），花费时间297.93秒（近5分钟）。

需要指出的是，readline函数每次只读取一行，因此内存消耗较小，可用于读取大文件并分行处理，这在数据清洗时有着重要的应用。但其劣势是需要较多的循环，而Python处理循环的速度较慢，因此很多时候我们往往是利用Numba、Cython等来进行性能优化。

此外，在Linux操作系统中，我们很容易利用shell命令来执行以上操作，具体方法是：
- head simulation_data.txt
- wc simulation_data.txt

# 处理数据

接下来，我们利用pandas中read_table来进行逐块读取文件，对分块文件进行处理，最后将结果进行合并。

对于分块数据的处理方法是将秒级别的行情数据转化为5分钟级别的分钟数据。此处我们假设1s内的open、high、low、close差别不大，因此转化的过程中我们主要用到了open。


```python
time1=time.time()
result=[]
reader = pd.read_table("simulation_data.txt",iterator=True,header=None)
loop=True
chunk_size=30*24*60*60*5
i=0
while loop:
    try:
        df=reader.get_chunk(chunk_size)
        df.columns=['datetime','stock_code','open','high','low','close']
        df['datetime']=pd.to_datetime(df.datetime,format="%Y-%m-%d %H:%M:%S")
        df['stock_code'] = df.stock_code.astype(np.int32)
        df['open'] = df.open.astype(np.float32)
        df['high'] = df.high.astype(np.float32)
        df['low'] = df.low.astype(np.float32)
        df['close'] = df.close.astype(np.float32)
        df=df.set_index(df.datetime)
        df=df.groupby(by=df.stock_code)
        df=df.open
        df=df.resample('5min',how="ohlc")
        df=df.reset_index()
        result.append(df)
        print(i)
        i=i+1
    except StopIteration:
        loop=False
        print("Iteration is stopped.")
result=pd.concat(result,axis=0,ignore_index=True)
result.to_csv("test1.txt",sep="\t",index=False)  
time2=time.time()
print("time:%.2f s" %(time2-time1))  
```

    C:\Anaconda3\lib\site-packages\pandas\core\groupby.py:1232: FutureWarning: how in .resample() is deprecated
    the new syntax is .resample(...).ohlc()
      return get_resampler_for_grouping(self, rule, *args, **kwargs)
    

    0
    1
    2
    3
    4
    5
    6
    7
    8
    9
    10
    11
    12
    13
    14
    15
    16
    17
    18
    19
    Iteration is stopped.
    time:718.45 s
    

对上述例子运行所花费的时间为718.45秒（12分钟）。汇总后的5分钟级别的行情数据大小为26.4 MB。


```python
result.info(memory_usage='deep')
result.head()
```

# 通过更改数据类型压缩数据

将23.5G的秒级别的行情数据汇总整理为26.4 MB的5分钟级别的行情数据我们花费了12分钟时间，并将数据压缩了近1000倍（23.5*1024/26.4=911.5）。这主要得益于两方面的改进：

- 将秒级别数据转化为5分钟级别数据（压缩倍数为300倍）
- 转换了数据类型

接下来，我们主要谈论在pandas中的DataFrame如何通过更改数据类型来压缩数据。

DataFrame中的数据类型主要由以下三种:
- int(包括int64、int32等)
- float(包括float64、float32等)
- object

通常情况下，int和float占用的空间是固定的（比如float64占用8字节，float32占用4字节），而object占用的空间不确定且占用空间较大。这主要是由于object在DataFrame中的存储方式造成的。因此我们在处理DataFrame时，要合理使用数据类型以减少数据所占的空间。主要方法由以下几种:
- 将float64、int64转化为float32、int32
- 将object转化为category
- 将object转化为datatime64

加下来我们依次介绍。

首先，我们读取一块数据（30*24*60*60*5条记录）,并查看数据类型以及所占用的空间。


```python
reader = pd.read_table("simulation_data.txt",iterator=True,header=None)
chunk_size=30*24*60*60*5
df=reader.get_chunk(chunk_size)
df.columns=['datetime','stock_code','open','high','low','close']
df.info(memory_usage='deep')
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 12960000 entries, 0 to 12959999
    Data columns (total 6 columns):
    datetime      object
    stock_code    int64
    open          float64
    high          float64
    low           float64
    close         float64
    dtypes: float64(4), int64(1), object(1)
    memory usage: 1.4 GB
    

有以上结果我们发现，df占用空间为1.4G。

通过下面的方法我们可以查看每种类型的数据所占的空间：


```python
###查看各个数据类型的平均内存用量   
for dtype in np.unique(df.dtypes):
    selected_dtype = df.select_dtypes(include=[dtype])
    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()
    mean_usage_mb = mean_usage_b / 1024 ** 2
    print("Average memory usage for {} columns: {:03.2f} MB".format(dtype,mean_usage_mb))
```

    Average memory usage for int64 columns: 49.44 MB
    Average memory usage for float64 columns: 79.10 MB
    Average memory usage for object columns: 469.67 MB
    

从以上结果可以看出，object所占的空间是int64的近10倍。因此，我们来更改数据类型，并查看更改后的结果。


```python
df['datetime']=pd.to_datetime(df.datetime,format="%Y-%m-%d %H:%M:%S")
df['stock_code'] = df.stock_code.astype(np.int32)
df['open'] = df.open.astype(np.float32)
df['high'] = df.high.astype(np.float32)
df['low'] = df.low.astype(np.float32)
df['close'] = df.close.astype(np.float32)
df.info(memory_usage='deep')
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 12960000 entries, 0 to 12959999
    Data columns (total 6 columns):
    datetime      datetime64[ns]
    stock_code    int32
    open          float32
    high          float32
    low           float32
    close         float32
    dtypes: datetime64[ns](1), float32(4), int32(1)
    memory usage: 346.1 MB
    

从上面的结果可以看出，更改数据类型后df由原来的1.4G变为346.1 MB。

** 参考文献: **

[dataquest的博客](https://www.dataquest.io/blog/pandas-big-data/)
